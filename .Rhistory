set.seed(375)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pallette)
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=palette)
wordFreq
wordFreq[1]
wordFreq <- wordFreq > 2000
wordFreq
wordFreq <- sort(rowSums(m), decreasing = TRUE)
wordFreq <- wordFreq[wordFreq > 2000]
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pallette)
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=palette)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud(words = names(wordFreq), freq=wordFreq, min.freq=2000, random.order=F)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud(words = names(wordFreq), freq=wordFreq, min.freq=2000, random.order=F)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud(words = names(wordFreq), freq=wordFreq, random.order=F)
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
email_data <- read.csv("https://raw.githubusercontent.com/wsjdata/clinton-email-cruncher/d8dc1916465b90e4147460f9e432cf9cafc8d3b5/HRCEMAIL_names.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
email_data <- read.csv("https://raw.githubusercontent.com/wsjdata/clinton-email-cruncher/d8dc1916465b90e4147460f9e432cf9cafc8d3b5/HRCEMAIL_names.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
email_data <- read.csv("https://www.kaggle.com/kaggle/hillary-clinton-emails?select=Emails.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
head(email_data)
numRows(email_data)
numrows(email_data)
nrow(email_data)
View(emailCorpus)
View(emailCorpusFrame)
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
View(emailCorpusFrame)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
View(emailCorpusFrame)
nrow(emailCorpusFrame)
emailCorpusFrame$text[1]
shiny::runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
totalEmails <- nrow(emailCorpusFrame)
allEmails <- emailCorpusFrame$text
supportNumber = 0
for (email in allEmails) {
check <- grepl("will", email, ignore.case = TRUE)
if (check) supportNumber = supportNumber + 1
}
class(allEmails)
inspect(allEmails)
allEmails <- emailCorpusFrame[,1]
class(allEmails)
df <- as.data.frame(emailCorpusFrame)
allEmails <- df$text
class(allEmails)
df<-data.frame(text=unlist(sapply(emailCorpus, `[`, "content")), stringsAsFactors=F)
allEmails <- df$text
class(df)
class(allEmails)
allEmails[1]
allEmails[2]
view(allEmails)
View(allEmails)
df<-data.frame(text=unlist(sapply(emailCorpusFrame, `[`, "content")), stringsAsFactors=F)
View(emailCorpus)
df<-data.frame(text=unlist(sapply(emailCorpus, `[`, "content")), stringsAsFactors=F)
df[1,]
df[1,1]
df[,1]
allEmails <- as.string(emailCorpusFrame$text)
allEmails <- as.String(emailCorpusFrame$text)
class(allEmails)
allEmails[1]
allEmails[1,1,1]
allEmails[1,1]
allEmails[1,2]
allEmails[1,6]
allEmails <- as.vector(emailCorpusFrame$text)
class(allEmails)
allEmails[1]
for (email in allEmails) {
check <- grepl("will", email, ignore.case = TRUE)
if (check) supportNumber = supportNumber + 1
}
supportNumber
dim(allEmails)
length(allEmails)
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
View(tdm)
View(tdm[1,2])
View(tdm[1,1])
View(tdm[1])
View(emailCorpusFrame)
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
library(shiny)
library(shinyWidgets)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
library(dplyr)
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
term_stats(emailCorpusFrame, ngrams=5)
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
emailCorpusFrame <- corpus_frame(emailCorpus)
emailCorpusFrame <- as_corpus_frame(emailCorpus)
term_stats(emailCorpusFrame, ngrams=5)
x<- term_stats(emailCorpusFrame, ngrams=5)[20]
x<- term_stats(emailCorpusFrame, ngrams=5)[20,]
x
x<- term_stats(emailCorpusFrame, ngrams=5)[1:20,]
class(x)
x
save(email_corpus_frame, fit, tdm, wordFreq, grayLevels, totalEmails, allEmails, file = "objects.RData")
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
library(dplyr)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
emailCorpusFrame <- as_corpus_frame(emailCorpus)
term_stats(emailCorpusFrame, ngrams=5)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm2 <- removeSparseTerms(tdm, 0.9)
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm2)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
# Association rule code
totalEmails <- nrow(emailCorpusFrame)
allEmails <- as.vector(emailCorpusFrame$text)
# Cluster terms
distMatrix <- dist(scale(m))
fit <- hclust(distMatrix, method = "ward.D")
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
groups <- cutree(fit, k=10)
group_frame <- data.frame(names(groups), groups[names(groups)])
colnames(group_frame) <- c("word", "group")
k = 10
currentFrame <- data.frame(1:k)
for (indx in 1:k) {
group <- (filter(group_frame, group == k))$words
currentWords = group$word
}
# sapply("mrkdwn.Rmd", knit, quiet = T)
save(emailCorpusFrame, fit, tdm, wordFreq, grayLevels, totalEmails, allEmails, file = "objects.RData")
runApp('D:/Code/DataMiningProject')
runApp('D:/Code/DataMiningProject')
as.table(x)
class(x)
x <- data.frame(text = sapply(x, as.character), stringsAsFactors = FALSE)
class(x)
as.table(x)
runApp('D:/Code/DataMiningProject')
runApp('D:/Code/DataMiningProject')
x
x$text.term
class(x)
runApp('D:/Code/DataMiningProject')
runApp('D:/Code/DataMiningProject')
runApp('D:/Code/DataMiningProject')
getwd()
setwd("D:/Code/DataMiningProject")
emailCorpusFrame <- as_corpus_frame(emailCorpus)
save(emailCorpusFrame, fit, tdm, wordFreq, grayLevels, totalEmails, allEmails, file = "objects.RData")
emailCorpus <- tm_map(emailCorpus, stemDocument)
emailCorpusFrame <- as_corpus_frame(emailCorpus)
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
emailCorpus <- tm_map(emailCorpus, stemDocument)
emailCorpusFrame <- as_corpus_frame(emailCorpus)
save(emailCorpusFrame, fit, tdm, wordFreq, grayLevels, totalEmails, allEmails, file = "objects.RData")
head(stopwords("en"))
(stopwords("en"))
stopwords("en")
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
emailCorpus <- tm_map(emailCorpus, removeURL)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
emailCorpus <- tm_map(emailCorpus, tolower)
emailCorpus <- tm_map(emailCorpus, removeURL)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
