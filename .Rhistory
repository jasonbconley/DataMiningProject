# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
email_data <- read.csv("https://raw.githubusercontent.com/wsjdata/clinton-email-cruncher/d8dc1916465b90e4147460f9e432cf9cafc8d3b5/HRCEMAIL_names.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
email_data <- read.csv("https://www.kaggle.com/kaggle/hillary-clinton-emails?select=Emails.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
head(email_data)
numRows(email_data)
numrows(email_data)
nrow(email_data)
View(emailCorpus)
View(emailCorpusFrame)
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
View(emailCorpusFrame)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
View(emailCorpusFrame)
nrow(emailCorpusFrame)
emailCorpusFrame$text[1]
shiny::runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
totalEmails <- nrow(emailCorpusFrame)
allEmails <- emailCorpusFrame$text
supportNumber = 0
for (email in allEmails) {
check <- grepl("will", email, ignore.case = TRUE)
if (check) supportNumber = supportNumber + 1
}
class(allEmails)
inspect(allEmails)
allEmails <- emailCorpusFrame[,1]
class(allEmails)
df <- as.data.frame(emailCorpusFrame)
allEmails <- df$text
class(allEmails)
df<-data.frame(text=unlist(sapply(emailCorpus, `[`, "content")), stringsAsFactors=F)
allEmails <- df$text
class(df)
class(allEmails)
allEmails[1]
allEmails[2]
view(allEmails)
View(allEmails)
df<-data.frame(text=unlist(sapply(emailCorpusFrame, `[`, "content")), stringsAsFactors=F)
View(emailCorpus)
df<-data.frame(text=unlist(sapply(emailCorpus, `[`, "content")), stringsAsFactors=F)
df[1,]
df[1,1]
df[,1]
allEmails <- as.string(emailCorpusFrame$text)
allEmails <- as.String(emailCorpusFrame$text)
class(allEmails)
allEmails[1]
allEmails[1,1,1]
allEmails[1,1]
allEmails[1,2]
allEmails[1,6]
allEmails <- as.vector(emailCorpusFrame$text)
class(allEmails)
allEmails[1]
for (email in allEmails) {
check <- grepl("will", email, ignore.case = TRUE)
if (check) supportNumber = supportNumber + 1
}
supportNumber
dim(allEmails)
length(allEmails)
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
library(tm)
library(corpus)
library(SnowballC) # love lover lovingly lovely loving
library(Rcpp)
library(ggplot2)
library(wordcloud)
library(dplyr) # https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
# Book: https://drive.google.com/file/d/1gn7cMdpMkDwHVTfDldAkn5i3_pRtoH-H/view
# Here we set up the fraudulent email data set into a Corpus for processing
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
# Corpus functionality requires a column be named "text"
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
emailCorpus <- tm_map(emailCorpus, removeURL)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud2(data=enframe(wordFreq[1:20]), ellipticity = 1, minRotation = -pi/6, maxRotation = -pi/6)
?hclust
distMatrix <- dist(scale(m))
fit <- hclust(distMatrix, method = "ward.D") # Will go into this in the presentation, hierarchical clustering, not using Ward
view(distMatrix)
View(distMatrix)
head(distMatrix)
scale(m)
?scale
?dist
# Cluster terms and plot
distMatrix <- dist(scale(m))
fit <- hclust(distMatrix, method = "ward.D") # Will go into this in the presentation, hierarchical clustering, not using Ward
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
groups <- cutree(fit, k=10)
library(tm)
library(corpus)
library(SnowballC) # love lover lovingly lovely loving
library(Rcpp)
library(ggplot2)
library(wordcloud2)
library(dplyr) # https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
# Book: https://drive.google.com/file/d/1gn7cMdpMkDwHVTfDldAkn5i3_pRtoH-H/view
# Here we set up the fraudulent email data set into a Corpus for processing
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
# Corpus functionality requires a column be named "text"
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
emailCorpus <- tm_map(emailCorpus, removeURL)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also", "available", "via")
myStopwords <- c(stopwords("en"), conjunctions) # stopwords("en")
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
emailCorpus <- tm_map(emailCorpus, stemDocument)
emailCorpusFrame <- as_corpus_frame(emailCorpus)
term_stats(emailCorpusFrame, ngrams=5)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud2(data=enframe(wordFreq[1:20]), ellipticity = 1, minRotation = -pi/6, maxRotation = -pi/6)
# Association rule code
totalEmails <- nrow(emailCorpusFrame)
allEmails <- as.vector(emailCorpusFrame$text)
#getSupport <- function(wordChoices, suppCount) {
#  supportNumber <- 0
#
#  for (email in allEmails) {
#    found = TRUE
#    for (word in wordChoices) {
#      check <- grepl(word, email, ignore.case = TRUE)
#      if (check == 0) {
#        found = FALSE
#        break
#      }
#    }
#    if (found) {
#      supportNumber = supportNumber + 1
#    }
#  }
#
#  supportNumber
# }
#
# Cluster terms and plot
distMatrix <- dist(scale(m))
fit <- hclust(distMatrix, method = "ward.D") # Will go into this in the presentation, hierarchical clustering, not using Ward
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
library(tm)
library(corpus)
library(SnowballC) # love lover lovingly lovely loving
library(Rcpp)
library(ggplot2)
library(wordcloud2)
library(dplyr) # https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
# Book: https://drive.google.com/file/d/1gn7cMdpMkDwHVTfDldAkn5i3_pRtoH-H/view
# Here we set up the fraudulent email data set into a Corpus for processing
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
# Corpus functionality requires a column be named "text"
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
emailCorpus <- tm_map(emailCorpus, removeURL)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "us", "may", "that", "and", "can", "get", "make", "shall", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also", "available", "via")
myStopwords <- c(stopwords("en"), conjunctions) # stopwords("en")
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
emailCorpus <- tm_map(emailCorpus, stemDocument)
emailCorpusFrame <- as_corpus_frame(emailCorpus)
term_stats(emailCorpusFrame, ngrams=5)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud2(data=enframe(wordFreq[1:20]), ellipticity = 1, minRotation = -pi/6, maxRotation = -pi/6)
# Association rule code
totalEmails <- nrow(emailCorpusFrame)
allEmails <- as.vector(emailCorpusFrame$text)
#getSupport <- function(wordChoices, suppCount) {
#  supportNumber <- 0
#
#  for (email in allEmails) {
#    found = TRUE
#    for (word in wordChoices) {
#      check <- grepl(word, email, ignore.case = TRUE)
#      if (check == 0) {
#        found = FALSE
#        break
#      }
#    }
#    if (found) {
#      supportNumber = supportNumber + 1
#    }
#  }
#
#  supportNumber
# }
#
# Cluster terms and plot
distMatrix <- dist(scale(m))
fit <- hclust(distMatrix, method = "ward.D") # Will go into this in the presentation, hierarchical clustering, not using Ward
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
groups <- cutree(fit, k=10)
group_frame <- data.frame(names(groups), groups[names(groups)])
colnames(group_frame) <- c("word", "group")
k = 10
currentFrame <- data.frame(1:k)
for (indx in 1:k) {
group <- (filter(group_frame, group == k))$words
currentWords = group$word
}
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
# sapply("mrkdwn.Rmd", knit, quiet = T)
save(emailCorpusFrame, fit, tdm, wordFreq, grayLevels, totalEmails, allEmails, file = "objects.RData")
getwd(\)
getwd()
setwd("D:\Code\DataMiningProject")
setwd("D:/Code/DataMiningProject")
save(emailCorpusFrame, fit, tdm, wordFreq, grayLevels, totalEmails, allEmails, file = "objects.RData")
?hclust
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
fit <- hclust(distMatrix, method = "ward.D") # Will go into this in the presentation, hierarchical clustering, not using Ward (1963) clustering criterion
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
plot(fit, xlab = "Words", sub = "Using Ward Clustering")
rect.hclust(fit, k=10)
