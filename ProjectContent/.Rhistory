library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
# Here we set up the fraudulent email dataset into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, removeWords)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# This function shows the frequency of terms that are greater than 10
findFreqTerms(tdm, lowfreq = 10)
install.packages("Rcpp")
findFreqTerms(tdm, lowfreq = 10)
findFreqTerms(tdm, lowfreq = 100)
findFreqTerms(tdm, lowfreq = 1000)
findFreqTerms(tdm, lowfreq = 1500)
findFreqTerms(tdm, lowfreq = 200)
findFreqTerms(tdm, lowfreq = 2000)
findFreqTerms(tdm, lowfreq = 2500)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
myStopwords <- c(stopwords(english), "available", "via")
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, removeWords)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# This function shows the frequency of terms that are greater than 10
findFreqTerms(tdm, lowfreq = 10)
findFreqTerms(tdm, lowfreq = 2000)
?stopwords
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this")
myStopwords <- c(stopwords(english), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, removeWords)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# This function shows the frequency of terms that are greater than 10
findFreqTerms(tdm, lowfreq = 2000)
?stopwords
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, removeWords)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# This function shows the frequency of terms that are greater than 10
findFreqTerms(tdm, lowfreq = 2000)
library(ggplot2)
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
class(tdm)
class(as.matrix(tdm))
as.matrix(tdm)
?rowSums
rowSums(as.matrix(tdm))
rowSums(as.numeric(as.matrix(tdm)))
df <- as.data.frame(tdm)
df <- as.matrix(tdm)
df
rowSums(df)
tdm
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
tdm
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
View(email_data)
?findFreqTerms
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# (View the matrix)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Trim less frequent words
removals <- findFreqTerms(tdm, highfreq = 1000)
tdm <- dtm_remove_terms(tdm, terms = removals)
?dtm_remove_terms
??dtm_remove_terms
install.packages("udpipe")
library(udpipe)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# (View the matrix)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Trim less frequent words
removals <- findFreqTerms(tdm, highfreq = 1000)
tdm <- dtm_remove_terms(tdm, terms = removals)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# (View the matrix)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Trim less frequent words
removals <- findFreqTerms(tdm, highfreq = 1000)
tdm <- as.TermDocumentMatrix(dtm_remove_terms(as.DocumentTermMatrix(tdm), terms = removals))
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
# tdm
# (View the matrix)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- rowSums(as.matrix(tdm)) > 1000
termFrequency <- rowSums(as.matrix(tdm)) > 2000
dim(tdm)
head(tdm)
View(tdm)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, removeWords)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(1,Inf)))
head(tdm)
View(tdm)
dim(tdm)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
dim(tdm)
?dfm_trm
?removeSparseTerms
dim(tdm)
tdm <- removeSparseTerms(tdm, 0.2)
dim(tdm)
View(tdm)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
dim(tdm)
removeSparseTerms(tdm, 0.2)
dim(tdm)
removeSparseTerms(tdm, 0.4)
dim(tdm)
removeSparseTerms(tdm, 0.1)
dim(tdm)
rowSums(tdm)
removeSparseTerms(tdm, 0.7)
dim(tdm)
termFrequency <- rowSums(as.matrix(tdm))
m <- as.matrix(tdm)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,10)))
m <- as.matrix(tdm)
rowSums(m)
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
m <- as.matrix(tdm)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,10)))
tdm
tdm <- removeSparseTerms(tdm, 0.5)
dim(tdm)
findFreqTerms(tdm, lowfreq = 2000)
findFreqTerms(tdm)
findFreqTerms(tdm, lowfreq = 2000)
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,10)))
tdm <- removeSparseTerms(tdm, 0.9)
findFreqTerms(tdm)
dim(tdm)
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
install.packages("wordcloud")
library(wordcloud)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing = TRUE)
pallete <- brewer.pal(9, "BuGn")
pallete <- pallete[-(1:4)]
set.seed(375)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pallette)
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=palette)
wordFreq
wordFreq[1]
wordFreq <- wordFreq > 2000
wordFreq
wordFreq <- sort(rowSums(m), decreasing = TRUE)
wordFreq <- wordFreq[wordFreq > 2000]
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pallette)
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=palette)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud(words = names(wordFreq), freq=wordFreq, min.freq=2000, random.order=F)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud(words = names(wordFreq), freq=wordFreq, min.freq=2000, random.order=F)
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
wordcloud(words = names(wordFreq), freq=wordFreq, random.order=F)
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
email_data <- read.csv("https://raw.githubusercontent.com/wsjdata/clinton-email-cruncher/d8dc1916465b90e4147460f9e432cf9cafc8d3b5/HRCEMAIL_names.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
email_data <- read.csv("https://raw.githubusercontent.com/wsjdata/clinton-email-cruncher/d8dc1916465b90e4147460f9e432cf9cafc8d3b5/HRCEMAIL_names.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
email_data <- read.csv("https://www.kaggle.com/kaggle/hillary-clinton-emails?select=Emails.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
head(email_data)
# Utilizing code and information from
# https://drive.google.com/file/d/1gn7cMdpMkDwHVTfDldAkn5i3_pRtoH-H/view
# a data mining book with a focus on the R programming language
# Only have run the below install functions once
install.packages("tm")
install.packages("corpus")
install.packages("SnowballC")
install.packages("ggplot2")
install.packages("Rcpp")
install.packages("wordcloud")
# Will most likely have to run below library functions every time project
# is reopened
library(tm)
library(corpus)
library(SnowballC)
library(Rcpp)
library(ggplot2)
library(wordcloud)
# Here we set up the fraudulent email data set into a Corpus for processing
# A corpus is a data structure that I had not seen before this text. Running
# the command View(emailCorpus) after creating it will show the structure
email_data <- read.csv("https://raw.githubusercontent.com/jasonbconley/DataMiningProject/main/fraud_email_.csv")
colnames(email_data) <- c("text", "class")
emailCorpusFrame <- corpus_frame(email_data)
emailCorpus <- Corpus(VectorSource(emailCorpusFrame$text))
# Here we strip the data of the things like whitespace and change all
# characters to lower case. Essentially so that the tm dataset can work with
# the data frame
emailCorpus <- tm_map(emailCorpus, tolower)
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeURL)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
emailCorpus <- tm_map(emailCorpus, removeNumPunct)
replaceComma <- function(x) gsub(",", " ", x)
emailCorpus <- tm_map(emailCorpus, replaceComma)
conjunctions <- c("a", "that", "and", "are", "as", "at", "but", "for", "has", "the", "is", "to", "or", "this", "come", "also")
myStopwords <- c(stopwords("en"), "available", "via", conjunctions)
emailCorpus <- tm_map(emailCorpus, removeWords, myStopwords)
emailCorpus <- tm_map(emailCorpus, removePunctuation)
emailCorpus <- tm_map(emailCorpus, removeNumbers)
emailCorpus <- tm_map(emailCorpus, stripWhitespace)
# Now we can create the term document matrix, a collection of all terms and
# the number of their occurrences in each document or email
tdm <- TermDocumentMatrix(emailCorpus, control=list(wordLengths=c(2,Inf)))
tdm <- removeSparseTerms(tdm, 0.9)
# This function shows the frequency of terms that are greater than 2000
# findFreqTerms(tdm, lowfreq = 2000)
# Create a word cloud of the most popular words
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=2000)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip()
# Setting some colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# Creating a word cloud, we will use this to show the user the most frequent words
m <- as.matrix(tdm)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(123)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=pal)
wordFreq
shiny::runApp('D:/Code/DataMiningProject/EmailFraud')
install.packages("shiny")
install.packages("shiny")
shiny::runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
runApp('D:/Code/DataMiningProject/EmailFraud')
